corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
controlList = list(removePunctuation = TRUE, removeNumbers = TRUE, stopwords = TRUE)
unigramTdm <- TermDocumentMatrix(en_news_corpus, control = c(controlList,list(tokenize = unigramTokenizer)))
bigramTdm <- TermDocumentMatrix(en_news_corpus, control = c(controlList,list(tokenize = bigramTokenizer)))
trigramTdm <- TermDocumentMatrix(en_news_corpus, control = c(controlList,list(tokenize = trigramTokenizer)))
findFreqTerms(bigramTdm, 10)
freq <- colSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)]
freq[head(ord)]
con <- file("~/final/en_US/en_US.news.txt", "r")
en_news<-readLines(con, 100) ## Read the first line of text
close(con)
#Tokenization using stylo
library(stylo)
tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
library("RWeka")
library("tm")
fileConn<-file("en_news_1.txt")
writeLines(en_news, fileConn)
close(fileConn)
docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(corpus,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(en_news_corpus, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(en_news_corpus, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(en_news_corpus, control = list(tokenize = trigramTokenizer))
freq <- colSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
freq[head(ord)]
findFreqTerms(unigramTdm, 10)
findFreqTerms(bigramTdm, 10)
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- colSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)]
con <- file("~/final/en_US/en_US.news.txt", "r")
en_news<-readLines(con, 100) ## Read the first line of text
close(con)
#Tokenization using stylo
library(stylo)
tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
library("RWeka")
library("tm")
fileConn<-file("en_news_1.txt")
writeLines(en_news, fileConn)
close(fileConn)
docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- colSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)]
docs
inspect(docs)
inspect(trigramTokenizer)
inspect(trigramTdm)
freq[head(ord)]
freq <- colSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)]
freq[tail(ord)]
unigramTdm
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)]
freq[tail(ord)]
head (table(freq), 15)
tail(table(freq),15)
plot(head (table(freq), 15))
plot(head (table(freq), 15))
tail(table(freq),15)
?plot
l(ord)]
plot(head (table(freq), 15),type="l",main="Most frequent words")
plot(tail(table(freq),15),type="l",main="Least frequent words")
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
head(freq)
tail(freq)
ord<-order(freq)
head(ord)
tail(ord)
freq[head(ord)]
freq[tail(ord)]
freq
table(freq)
freq
table(freq)
plot(head (table(freq), 15),type="l",main="Most frequent words",xlab="Number of occurrences")
plot(tail(table(freq),15),type="l",main="Least frequent words",xlab="Number of occurrences")
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
findFreqTerms(unigramTdm, 10)
findFreqTerms(unigramTdm, 250)
findFreqTerms(bigramTdm, 10)
findFreqTerms(trigramTdm, 20)
findFreqTerms(trigramTdm, 2)
plot(unigramTdm, terms = findFreqTerms(unigramTdm, lowfreq = 6)[1:10], corThreshold = 0.5)
plot(bigramTdm, terms = findFreqTerms(bigramTdm, lowfreq = 6)[1:10], corThreshold = 0.5)
con <- file("~/final/en_US/en_US.news.txt", "r")
en_news<-readLines(con) ## Read the first line of text
close(con)
#Tokenization using stylo
library(stylo)
tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
library("RWeka")
library("tm")
fileConn<-file("en_news_1.txt")
writeLines(en_news, fileConn)
close(fileConn)
docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
#plot(head (table(freq), 15),type="l",main="Most frequent words",xlab="Number of occurrences")
#plot(tail(table(freq),15),type="l",main="Most frequent words",xlab="Number of occurrences")
inspect(bigramTdm)
findFreqTerms(unigramTdm, 250)
findFreqTerms(bigramTdm, 10)
findFreqTerms(trigramTdm, 2)
#en_news <- readLines('~/final/en_US/en_US.news.txt');
#con <- file("~/final/en_US/en_US.news.txt", "r")
#en_news<-readLines(con) ## Read the first line of text
#close(con)
#Tokenization using stylo
#library(stylo)
#tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
#unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
#plot(head (table(freq), 15),type="l",main="Most frequent words",xlab="Number of occurrences")
#plot(tail(table(freq),15),type="l",main="Most frequent words",xlab="Number of occurrences")
inspect(bigramTdm)
findFreqTerms(unigramTdm, 250)
findFreqTerms(bigramTdm, 10)
findFreqTerms(trigramTdm, 2)
#Tokenization using stylo
#library(stylo)
#tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
#unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
options( java.parameters = "-Xmx8g" )
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
#plot(head (table(freq), 15),type="l",main="Most frequent words",xlab="Number of occurrences")
#plot(tail(table(freq),15),type="l",main="Most frequent words",xlab="Number of occurrences")
inspect(bigramTdm)
findFreqTerms(unigramTdm, 250)
findFreqTerms(bigramTdm, 10)
findFreqTerms(trigramTdm, 2)
findFreqTerms(unigramTdm, 250)
findFreqTerms(unigramTdm, 2500)
findFreqTerms(unigramTdm, 25000)
findFreqTerms(bigramTdm, 1000)
findFreqTerms(bigramTdm, 10000)
findFreqTerms(bigramTdm, 8000)
findFreqTerms(trigramTdm, 2)
findFreqTerms(trigramTdm, 20)
findFreqTerms(trigramTdm, 200)
Zipf_plot(bigramTdm, type = "l")
Heaps_plot(bigramTdm, type = "l")
txt <- readLines(con,200)
con <- file("~/final/en_US/en_US.twitter.txt", "r")
#con <- file("~/final/en_US/en_US_blogs.txt","r")
#en_news<-readLines(con) ## Read the first line of text
txt <- readLines(con,200)
#close(con)
#Tokenization using stylo
#library(stylo)
#tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
#unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
options( java.parameters = "-Xmx8g" )
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
docs<-PCorpus(txt)
#docs<-Corpus(DirSource("corpus"))
dbControl = list(dbName = "corpus/pcorpus.db", dbType = "DB1"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
docs<-PCorpus(VectorSource(txt),dbControl = list(dbName = "corpus/pcorpus.db", dbType = "DB1"))
install.package("filehast")
install.package("filehash")
install.packages("filehash")
docs<-PCorpus(VectorSource(txt),dbControl = list(dbName = "corpus/pcorpus.db", dbType = "DB1"))
inspect(docs)
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
inspect(bigramTdm)
findFreqTerms(unigramTdm, 25000)
findFreqTerms(bigramTdm, 8000)
findFreqTerms(trigramTdm, 200)
findFreqTerms(unigramTdm, 25)
Zipf_plot(bigramTdm, type = "l")
inspect(bigramTdm[1:10,1:10])
con <- file("~/final/en_US/en_US.twitter.txt", "r")
#con <- file("~/final/en_US/en_US_blogs.txt","r")
#en_news<-readLines(con) ## Read the first line of text
txt <- readLines(con,2000)
#close(con)
#Tokenization using stylo
#library(stylo)
#tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
#unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
options( java.parameters = "-Xmx8g" )
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
docs<-Corpus(VectorSource(txt))
#docs<-Corpus(DirSource("corpus"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
freq <- rowSums(as.matrix(unigramTdm))
length(freq)
ord <- order(freq)
# Least frequent terms
freq[head(ord)] #Least frequent words
freq[tail(ord)] #Most frequent words
txt
con <- file("~/final/en_US/en_US.twitter.txt", "r")
#con <- file("~/final/en_US/en_US_blogs.txt","r")
#en_news<-readLines(con) ## Read the first line of text
txt <- readLines(con,2000)
#close(con)
#Tokenization using stylo
#library(stylo)
#tokens<- txt.to.words(en_news, splitting.rule = NULL, preserve.case = FALSE)
#unigrams <- make.ngrams(en_news, ngram.size = 1)
#Tokenization using RWeka
options(mc.cores=1)
options( java.parameters = "-Xmx8g" )
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
docs<-Corpus(VectorSource(txt),encoding="UTF-8"), readerControl = list(language="en_US"))
docs<-Corpus(VectorSource(txt,encoding="UTF-8"), readerControl = list(language="en_US"))
options(mc.cores=1)
options( java.parameters = "-Xmx8g" )
library("RWeka")
library("tm")
#fileConn<-file("en_news_1.txt")
#writeLines(en_news, fileConn)
#close(fileConn)
#docs<-Corpus(VectorSource(txt)
docs<-Corpus(DirSource("corpus",encoding="UTF-8"), readerControl = list(language="en_US"))
docs <- tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs,removeNumbers)
docs <- tm_map(docs,removePunctuation)
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs, removeWords, stopwords ("english"))
docs <- tm_map(docs,stripWhitespace)
docs <- tm_map(docs,stemDocument)
#corpus<-removeSparseTerms(corpus, 0.2)
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1,delimiters = " \\r\\n\\t.,;:\"()?!"))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2,delimiters = " \\r\\n\\t.,;:\"()?!"))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3,delimiters = " \\r\\n\\t.,;:\"()?!"))
unigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = unigramTokenizer))
bigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = bigramTokenizer))
trigramTdm <- TermDocumentMatrix(docs, control = list(tokenize = trigramTokenizer))
findFreqTerms(trigramTdm, 2000)
inspect(unigramTdm)
unigramTdm$meta
meta(unigramTdm)
save(unigramTdm,"corpus/unigramTdm.RData")
save(unigramTdm,file="corpus/unigramTdm.RData")
save(bigramTdm,file="corpus/bigramTdm.RData")
save(trigramTdm,file="corpus/trigramTdm.RData")
save(docs,file="corpus/docs.RData")
---
title: "P1_Wine"
author: "Huseyin Coskun/Ruben Garzon"
date: "15 Nov 2014"
output: html_document
---
We will first read the Dataset obtained from UCI ML repository
https://archive.ics.uci.edu/ml/datasets/Wine
```{r}
wine <- read.csv("../Datasets/Wine/wine.data", header=FALSE)
colnames(wine) <- c("class","Alcohol","Malic Acid","Ash","Alcalinity of Ash","Magnesium","Total Phenols","Flavanoids","Nonflavanoid Phenols","Proanthocyanins","Color Intensity","Hue","0D280/OD315 of Diluted Wines","Proline")
```
We can plot the data with PCA to explore the distribution of classes (UNIFINISHED)
```{r}
# log transform
log.wine <- log(wine[, 2:14])
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
wine.pca <- prcomp(log.wine,
center = TRUE,
scale. = TRUE)
summary(wine.pca)
plot(wine.pca, type = "l")
library(rgl)
plot3d(wine.pca$scores[,1:3], col=wine$class)
setwd('/Users/rubengarzon/Google Drive/Shared/CI/Proposal1')
wine <- read.csv("../Datasets/Wine/wine.data", header=FALSE)
colnames(wine) <- c("class","Alcohol","Malic Acid","Ash","Alcalinity of Ash","Magnesium","Total Phenols","Flavanoids","Nonflavanoid Phenols","Proanthocyanins","Color Intensity","Hue","0D280/OD315 of Diluted Wines","Proline")
wine <- read.csv("../Datasets/Wine/wine.data", header=FALSE)
log.wine <- log(wine[, 2:14])
# apply PCA - scale. = TRUE is highly
# advisable, but default is FALSE.
wine.pca <- prcomp(log.wine,
center = TRUE,
scale. = TRUE)
summary(wine.pca)
plot(wine.pca, type = "l")
library(rgl)
plot3d(wine.pca$scores[,1:3], col=wine$class)
wine$class
win
wine
plot3d(wine.pca$scores[,1:3], col=wine[,1])
wine.pca$scores
str(wine.pca)
wine.pca <- prcomp(log.wine,
center = TRUE,
scale. = TRUE,cor=TRUE,scores=TRUE)
summary(wine.pca)
plot(wine.pca, type = "l")
library(rgl)
plot3d(wine.pca$scores[,1:3], col=wine[,1])
wine.pca$scores
wine.pca
wine.pca[,1:3]
wine.pca$rotations[,1:3]
str(wine.pca)
wine.pca <- princomp(log.wine,
center = TRUE,
scale. = TRUE,cor=TRUE,scores=TRUE)
summary(wine.pca)
plot(wine.pca, type = "l")
library(rgl)
plot3d(wine.pca$scores[,1:3], col=wine[,1])
We apply SVM for prototype selection. We will choose the support vectors as prototypes, although some improvement could be done.
__PENDING__ Adjust type of kernel used, cost parameter.
library(caret)
inTrain <- createDataPartition(wine$class, p=0.6, list=FALSE)
